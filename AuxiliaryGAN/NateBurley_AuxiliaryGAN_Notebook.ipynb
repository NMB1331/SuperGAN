{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will test the effectiveness of the Auxiliary GAN method for generating synthetic seizure data. It will be assessed using the TSTR method.\n",
    "\n",
    "Notes/To do:\n",
    "- Fix the feature_net to make it output the right shape. Compare it to the SuperGAN one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "import training_module as train\n",
    "import conditional_training_module as cond_train\n",
    "import input_module as input_mod\n",
    "import saving_module as save\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "import sys, models, conditional_models, tensorflow.keras\n",
    "\n",
    "DATAFILE = \"/Users/nburley/gans_deep_learning/IMWUT_GAN/code/NateBurley_Research_GANs/data/Epileptic_Seizure_Recognition.csv\"\n",
    "NUM_OG_CLASSES = 5\n",
    "num_classes = 2\n",
    "\n",
    "# Global variables for training\n",
    "NUM_CLASSES = 2\n",
    "NUM_CLASSIFIER_EPOCHS = 12\n",
    "NUM_TSTR_CLASSIFIER_EPOCHS = 20\n",
    "NUM_GAN_EPOCHS = 40 # Formerly 25. Curious if RTS or SFD is more important\n",
    "CLASSIFIER_TRAIN_RATIO = 0.8\n",
    "NUM_SYNTHETIC_SAMPLES = 2000 # Per class\n",
    "NUM_STATS = 9 # Number of statistics computed for loss function. Do NOT change\n",
    "\n",
    "# PARAMETERS RELATED TO TRAINING\n",
    "latent_dim = 70 #length of random input fed to generator (should it be num_classes + 1?)\n",
    "epochs = 100 #num training epochs\n",
    "batch_size = 30 #num instances generated for G/D training\n",
    "test_size = 100 #num instances generated for validating data\n",
    "real_synthetic_ratio = 5 #num synthetic instances per real instance for computing RTS metric\n",
    "synthetic_synthetic_ratio = 10 #num synthetic instances to compare for computing STS metric\n",
    "disc_lr = .08 #learning rate of discriminator\n",
    "accuracy_threshold = 0.95 #threshold to stop generator training\n",
    "instances_per_class_train = 10\n",
    "instances_per_class_test = 10\n",
    "\n",
    "# WEIGHTS FOR DIFFERENT TERMS IN THE LOSS FUNCTION\n",
    "D_loss_weight = 1.25\n",
    "D_loss_weight2 = 1.25\n",
    "C_loss_weight = 0.75\n",
    "SFD_loss_weight = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data into pandas dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         1      2      3      4      5      6      7      8      9      10   \\\n",
      "4748   0.008  0.027  0.029  0.009 -0.032 -0.058 -0.075 -0.080 -0.079 -0.105   \n",
      "468   -0.003 -0.014 -0.022 -0.027 -0.031 -0.033 -0.030 -0.024 -0.019 -0.009   \n",
      "10289 -0.183 -0.232 -0.272 -0.315 -0.341 -0.378 -0.396 -0.425 -0.443 -0.469   \n",
      "7262  -0.057 -0.061 -0.054 -0.050 -0.051 -0.051 -0.066 -0.076 -0.081 -0.082   \n",
      "7504  -0.003 -0.008 -0.015 -0.023 -0.025 -0.023 -0.023 -0.017 -0.020 -0.023   \n",
      "\n",
      "       ...    169    170    171    172    173    174    175    176    177  \\\n",
      "4748   ... -0.022 -0.005 -0.007 -0.018 -0.031 -0.041 -0.053 -0.052 -0.045   \n",
      "468    ... -0.001 -0.002 -0.004 -0.006 -0.006 -0.005 -0.008 -0.014 -0.014   \n",
      "10289  ...  0.037  0.044  0.059  0.066  0.076  0.079  0.076  0.061  0.034   \n",
      "7262   ... -0.050 -0.039 -0.036 -0.035 -0.036 -0.033 -0.024 -0.014 -0.004   \n",
      "7504   ...  0.008 -0.005 -0.015 -0.020 -0.020 -0.018 -0.023 -0.022 -0.023   \n",
      "\n",
      "         178  \n",
      "4748  -0.036  \n",
      "468   -0.014  \n",
      "10289  0.009  \n",
      "7262   0.008  \n",
      "7504  -0.018  \n",
      "\n",
      "[5 rows x 178 columns]\n",
      "      1      2      3      4      5      6      7      8      9      10   ...  \\\n",
      "1   0.386  0.382  0.356  0.331  0.320  0.315  0.307  0.272  0.244  0.232  ...   \n",
      "8  -0.278 -0.246 -0.215 -0.191 -0.177 -0.167 -0.157 -0.139 -0.118 -0.092  ...   \n",
      "11 -0.167 -0.230 -0.280 -0.315 -0.338 -0.369 -0.405 -0.392 -0.298 -0.140  ...   \n",
      "20  0.410  0.451  0.491  0.541  0.581  0.641  0.736  0.757  0.692  0.435  ...   \n",
      "22 -0.264 -0.189 -0.117 -0.045  0.020  0.070  0.111  0.143  0.161  0.179  ...   \n",
      "\n",
      "      169    170    171    172    173    174    175    176    177    178  \n",
      "1   0.168  0.164  0.150  0.146  0.152  0.157  0.156  0.154  0.143  0.129  \n",
      "8  -0.386 -0.400 -0.379 -0.336 -0.281 -0.226 -0.174 -0.125 -0.079 -0.040  \n",
      "11  0.415  0.423  0.434  0.416  0.374  0.319  0.268  0.215  0.165  0.103  \n",
      "20  0.409  0.415  0.428  0.463  0.510  0.562  0.607  0.667  0.748  0.763  \n",
      "22 -0.227 -0.231 -0.221 -0.248 -0.321 -0.444 -0.530 -0.548 -0.536 -0.486  \n",
      "\n",
      "[5 rows x 178 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nburley/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4238: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().rename(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Read in the data; seperate by class\n",
    "eeg_raw = pd.read_csv(DATAFILE)\n",
    "class_df_dict = {}\n",
    "for y_val, eeg_df in eeg_raw.groupby('y'):\n",
    "    eeg_df.columns = [int(i) for i in range(1, (len(eeg_df.columns)+1))]\n",
    "    eeg_df.rename(columns={ eeg_df.columns[len(eeg_df.columns)-1]: \"y\" }, inplace=True)\n",
    "    class_df_dict[y_val] = eeg_df\n",
    "\n",
    "# Join non-seizure classes into one dataset\n",
    "#X_0_df = pd.concat([class_df_dict[x] for x in range(2, NUM_OG_CLASSES+1)])\n",
    "X_0_df = class_df_dict[2]\n",
    "X_0_df.loc[:,'y'] = 0\n",
    "Y_0_df = X_0_df['y']\n",
    "del X_0_df['y']\n",
    "X_0_df = X_0_df.apply(pd.to_numeric)\n",
    "X_0_df = X_0_df.sample(frac=1) # Shuffle rows\n",
    "\n",
    "# Normalize\n",
    "X_0_df = X_0_df / 1000\n",
    "print(X_0_df.head(5))\n",
    "\n",
    "# Build seizure dataset\n",
    "X_1_df = class_df_dict[1]\n",
    "X_1_df.loc[:,'y'] = 1\n",
    "Y_1_df = X_1_df['y']\n",
    "del X_1_df['y']\n",
    "X_1_df = X_1_df.apply(pd.to_numeric)\n",
    "X_0_df = X_0_df.sample(frac=1) # Shuffle rows\n",
    "\n",
    "# Normalize\n",
    "X_1_df = X_1_df / 1000\n",
    "print(X_1_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we convert the data to numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y shape: (4600, 1)\n",
      "X shape: (4600, 178, 1)\n",
      "\n",
      "Train X shape: (3680, 178, 1)\n",
      "Test X shape: (920, 178, 1)\n",
      "Train Y shape: (3680, 2)\n",
      "Test Y shape: (920, 2)\n",
      "\n",
      "Y_0 shape: (2300, 1)\n",
      "Y_1 shape: (2300, 1)\n",
      "X_0 shape: (2300, 178, 1)\n",
      "X_1 shape: (2300, 178, 1)\n",
      "\n",
      "In shape: (178, 1)\n"
     ]
    }
   ],
   "source": [
    "# Join them together\n",
    "Y_df = pd.concat((Y_0_df, Y_1_df))\n",
    "X_df = pd.concat((X_0_df, X_1_df))\n",
    "\n",
    "# Build X_0 and Y_0 datasets\n",
    "Y_0 = Y_0_df.values.T\n",
    "Y_0 = Y_0.reshape((Y_0.shape[0], 1))\n",
    "X_0 = X_0_df.values\n",
    "#X_0 = normalize(X_0)\n",
    "X_0 = X_0.reshape((X_0.shape[0], X_0.shape[1], 1))\n",
    "\n",
    "Y_1 = Y_1_df.values.T\n",
    "Y_1 = Y_1.reshape((Y_1.shape[0], 1))\n",
    "X_1 = X_1_df.values\n",
    "#X_1 = normalize(X_1)\n",
    "X_1 = X_1.reshape((X_1.shape[0], X_1.shape[1], 1))\n",
    "\n",
    "# Convert to numpy arrays, reshape\n",
    "Y = Y_df.values.T\n",
    "Y = Y.reshape((Y.shape[0], 1))\n",
    "X = X_df.values\n",
    "#X = normalize(X)\n",
    "X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "print(\"Y shape: {}\".format(Y.shape))\n",
    "print(\"X shape: {}\".format(X.shape))\n",
    "\n",
    "# Build training and testing sets\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(X, Y, \\\n",
    "    train_size=CLASSIFIER_TRAIN_RATIO, shuffle=True)\n",
    "print(\"\\nTrain X shape: {}\".format(train_X.shape))\n",
    "print(\"Test X shape: {}\".format(test_X.shape))\n",
    "\n",
    "# Convert the Y's to categorical\n",
    "Y = tensorflow.keras.utils.to_categorical(Y)\n",
    "Y_0 = tensorflow.keras.utils.to_categorical(Y_0)\n",
    "#Y_1 = keras.utils.np_utils.to_categorical(Y_1)\n",
    "train_Y = tensorflow.keras.utils.to_categorical(train_Y)\n",
    "test_Y = tensorflow.keras.utils.to_categorical(test_Y)\n",
    "print(\"Train Y shape: {}\".format(train_Y.shape))\n",
    "print(\"Test Y shape: {}\".format(test_Y.shape))\n",
    "print(\"\\nY_0 shape: {}\".format(Y_0.shape))\n",
    "print(\"Y_1 shape: {}\".format(Y_1.shape))\n",
    "print(\"X_0 shape: {}\".format(X_0.shape))\n",
    "print(\"X_1 shape: {}\".format(X_1.shape))\n",
    "\n",
    "# Parameters for training shape\n",
    "num_seqs = X.shape[0]\n",
    "seq_length = X.shape[1]\n",
    "num_channels = X.shape[2]\n",
    "input_shape = (seq_length, num_channels)\n",
    "print(\"\\nIn shape: {}\".format(input_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the generator model (since this architecture apparently doesn't use a classifier?), and then train the model (because who gives a fuck idc to split this up):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 178, 3)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 178, 3)       0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 100)          41600       dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            101         lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 2)            202         lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 41,903\n",
      "Trainable params: 41,903\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"SFN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 178, 1)]          0         \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 9)                 0         \n",
      "=================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Output layer summary: \n",
      "None\n",
      "Feature net prediction shape: (4600, 9)\n",
      "Feature net prediction shape: (4600, 9)\n",
      "Epoch: 1\n",
      "Class labels input: [[[1. 0.]\n",
      "  [1. 0.]\n",
      "  [1. 0.]\n",
      "  ...\n",
      "  [1. 0.]\n",
      "  [1. 0.]\n",
      "  [1. 0.]]\n",
      "\n",
      " [[1. 0.]\n",
      "  [1. 0.]\n",
      "  [1. 0.]\n",
      "  ...\n",
      "  [1. 0.]\n",
      "  [1. 0.]\n",
      "  [1. 0.]]\n",
      "\n",
      " [[1. 0.]\n",
      "  [1. 0.]\n",
      "  [1. 0.]\n",
      "  ...\n",
      "  [1. 0.]\n",
      "  [1. 0.]\n",
      "  [1. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 1.]\n",
      "  [0. 1.]\n",
      "  [0. 1.]\n",
      "  ...\n",
      "  [0. 1.]\n",
      "  [0. 1.]\n",
      "  [0. 1.]]\n",
      "\n",
      " [[0. 1.]\n",
      "  [0. 1.]\n",
      "  [0. 1.]\n",
      "  ...\n",
      "  [0. 1.]\n",
      "  [0. 1.]\n",
      "  [0. 1.]]\n",
      "\n",
      " [[0. 1.]\n",
      "  [0. 1.]\n",
      "  [0. 1.]\n",
      "  ...\n",
      "  [0. 1.]\n",
      "  [0. 1.]\n",
      "  [0. 1.]]]\n",
      "Real data shape: (20, 1)\n"
     ]
    },
    {
     "ename": "AxisError",
     "evalue": "axis 2 is out of bounds for array of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-2e9c57dc5c68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m#TRAIN DISCRIMINATOR AND GENERATOR AND DISPLAY ACCURACY FOR EACH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mD_loss_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcond_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_AC_D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstances_per_class_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_labels_input_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_labels_target_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_CLASSES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mGCD_loss_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcond_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_AC_G\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstances_per_class_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_labels_input_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_labels_target_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS_X_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_CLASSES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGCD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mD_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mD_loss_vec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#accuracy for discriminator during its \"turn\" for training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gans_deep_learning/IMWUT_GAN/code/NateBurley_Research_GANs/AuxiliaryGAN/conditional_training_module.py\u001b[0m in \u001b[0;36mtrain_AC_D\u001b[0;34m(instances_per_class, X, y, class_labels_input, class_labels_target, num_labels, generator, discriminator_model, seq_length, latent_dim)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Class labels input: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_labels_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Real data shape: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m     \u001b[0mreal_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclass_labels_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#originally axis=2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;31m#MAKE FULL INPUT AND LABELS FOR FEEDING INTO NETWORK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mAxisError\u001b[0m: axis 2 is out of bounds for array of dimension 2"
     ]
    }
   ],
   "source": [
    "#CREATE VECTOR OF LABELS USED FOR TRAINING C AND CONDITIONING G AND D\n",
    "class_labels_target_train = cond_train.create_target_label_vector_all_classes(instances_per_class_train, seq_length, num_classes)[0] #these are one-hot since they're fed to keras model\n",
    "class_labels_target_test = cond_train.create_target_label_vector_all_classes(instances_per_class_test, seq_length, num_classes)[1] #these are standard since sklearn evaluation methods are used\n",
    "class_labels_input_train = cond_train.create_input_label_vector_all_classes(instances_per_class_train, seq_length, num_classes)\n",
    "\n",
    "#CREATE GENERATOR AND DISCRIMINATOR\n",
    "G = conditional_models.create_cond_G(seq_length, num_channels, NUM_CLASSES, latent_dim)\n",
    "D = conditional_models.create_AC_D(seq_length, num_channels, NUM_CLASSES)\n",
    "D_to_freeze = D\n",
    "D_model = models.compile_discriminator_model(D, disc_lr)\n",
    "\n",
    "#CREATE STATISTICAL FEATURE NETWORK AND COMPUTE FEATURE VECTOR FOR REAL DATA (used in loss function)\n",
    "feature_net = models.create_statistical_feature_net(seq_length, num_channels)\n",
    "S_X_train, S_X_test = cond_train.compute_statistical_vector(X,Y,feature_net,num_channels,NUM_CLASSES,instances_per_class_train,instances_per_class_test)\n",
    "\n",
    "#CREATE FULL ARCHITECTURE WHERE OUPUT OF GENERATOR IS FED TO DISCRIMINATOR AND CLASSIFIER\n",
    "for layer in D_to_freeze.layers:\n",
    "    layer.trainable = False\n",
    "GCD = Model(inputs=G.input, outputs=[D_to_freeze(G.output[1])[0], D_to_freeze(G.output[1])[1],feature_net(G.output[0])])\n",
    "GCD.compile(loss=[\"binary_crossentropy\",\"categorical_crossentropy\", train.euc_dist_loss], \n",
    "            optimizer=\"adam\", metrics={\"D\":\"accuracy\"},\n",
    "            loss_weights = [D_loss_weight, D_loss_weight2 ,SFD_loss_weight])\n",
    "\n",
    "'''\n",
    "GCD.compile(loss={\"D\":\"binary_crossentropy\",\"C\":\"categorical_crossentropy\",\"SFN\": train.euc_dist_loss}, \n",
    "\t\t\toptimizer=\"adam\", metrics={\"D\":\"accuracy\",'C':\"accuracy\"},\n",
    "\t\t\tloss_weights = {\"D\": D_loss_weight, \"C\": C_loss_weight,\"SFN\": SFD_loss_weight})\n",
    "'''\n",
    "\n",
    "\n",
    "GC_acc=0\n",
    "epoch=1\n",
    "max_RTS = 0\n",
    "max_RTS_epoch = 0\n",
    "min_STS = 1000\n",
    "min_STS_epoch = 0\n",
    "min_SFD = 10000\n",
    "min_SFD_epoch = 0\n",
    "while (GC_acc<accuracy_threshold) and (epoch <= 100):\n",
    "    print(\"Epoch: \" + str(epoch))\n",
    "\n",
    "    #TRAIN DISCRIMINATOR AND GENERATOR AND DISPLAY ACCURACY FOR EACH\n",
    "    D_loss_vec = cond_train.train_AC_D(instances_per_class_train, X, Y, class_labels_input_train, class_labels_target_train, NUM_CLASSES, G, D_model, seq_length, latent_dim)\n",
    "    GCD_loss_vec = cond_train.train_AC_G(instances_per_class_train, X, class_labels_input_train, class_labels_target_train, S_X_train, NUM_CLASSES, GCD, seq_length, latent_dim)\n",
    "    D_acc = D_loss_vec[1] #accuracy for discriminator during its \"turn\" for training\n",
    "    GD_acc = GCD_loss_vec[4] #accuracy for generator in tricking discriminator\n",
    "    print(\"D Acc: \" + str(D_acc))\n",
    "    print(\"G Acc in tricking D: \" + str(GD_acc))\n",
    "\n",
    "    #GENERATE SYNTHETIC DATA AND FEED TO CLASSIFIER TO DETERMINE ACCURACY\n",
    "    synthetic_data = cond_train.generate_synthetic_data_all_classes(test_size, G, class_label, NUM_CLASSES, latent_dim, seq_length)\n",
    "#     pred = C.predict_classes(synthetic_data,test_size,verbose=0)\n",
    "#     #true = [class_label]*test_size\n",
    "#     GC_acc = accuracy_score(class_labels_target_test, pred)\n",
    "#     print(\"C acc for synthetic data: \" + str(GC_acc))\n",
    "\n",
    "    #COMPUTE RTS AND STS METRICS\n",
    "    mean_RTS_sim, mean_STS_sim, _ = train.compute_similarity_metrics(synthetic_data, X, test_size,real_synthetic_ratio, synthetic_synthetic_ratio)\n",
    "    print(\"RTS similarity: \" + str(mean_RTS_sim))\n",
    "    print(\"STS similarity: \" + str(mean_STS_sim))\n",
    "\n",
    "    #COMPUTE STATISTICAL FEATURE DISTANCE\n",
    "    synthetic_features = feature_net.predict(synthetic_data, test_size, verbose=0)\n",
    "    SFD = train.compute_SFD(synthetic_features, S_X_test)\n",
    "    print(\"SFD: \" + str(SFD))\n",
    "\n",
    "    #IF DESIRED, SAVE GENERTOR MODEL / WRITE TRAINING RESULTS\n",
    "    if model_save_directory!=False:\n",
    "        save.save_G(G, epoch, class_label, model_save_directory)\n",
    "    if write_train_results == True:\n",
    "        save.write_results(outfile, epoch, class_label, D_acc, GD_acc, GC_acc, mean_RTS_sim, mean_STS_sim)\n",
    "    \n",
    "    #RE-EVALUATE OUR TALLIES OF EPOCHS WITH LOWEST STS, HIGHEST RTS\n",
    "    if (mean_RTS_sim > max_RTS) and epoch >= 25:\n",
    "        max_RTS_epoch = epoch\n",
    "        max_RTS = mean_RTS_sim\n",
    "    if (mean_STS_sim < min_STS) and epoch >= 25:\n",
    "        min_STS_epoch = epoch\n",
    "        min_STS = mean_STS_sim\n",
    "    if (SFD < min_SFD) and epoch >= 25:\n",
    "        min_SFD_epoch = epoch\n",
    "        min_SFD = SFD\n",
    "        print(\"Updated SFD\")\n",
    "\n",
    "    epoch+=1\n",
    "    one_segment_real = np.reshape(X[np.random.randint(0, X.shape[0], 1)], (seq_length, num_channels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
